{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import demap\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hierarchical_umap as h_umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(X, X_emb):\n",
    "    \n",
    "    dist_orig = np.square(euclidean_distances(X, X)).flatten()\n",
    "    dist_emb = np.square(euclidean_distances(X_emb, X_emb)).flatten()\n",
    "    \n",
    "    \n",
    "    coef, p = spearmanr(dist_orig, dist_emb)\n",
    "    return coef\n",
    "\n",
    "def stress(X, X_emb):\n",
    "    \n",
    "    DE = euclidean_distances(X_emb)\n",
    "    DE = DE/np.max(DE)\n",
    "    DH = euclidean_distances(X)\n",
    "    DH = DH/np.max(DH)\n",
    "    stress = 0.5 * np.sum((DE - DH)**2)\n",
    "    \n",
    "    return np.sqrt(stress/(0.5*np.sum(DH**2)))\n",
    "    \n",
    "\n",
    "def neighborhood_preservation(X, X_emb, Khigh=30):\n",
    "    \n",
    "    neigh_high = NearestNeighbors(n_neighbors=Khigh+1, n_jobs=-1)\n",
    "    neigh_high.fit(X)\n",
    "    high_dists, high_indices = neigh_high.kneighbors(X)\n",
    "\n",
    "\n",
    "    neigh_emb = NearestNeighbors(n_neighbors=Khigh+1, n_jobs=-1)\n",
    "    neigh_emb.fit(X_emb)\n",
    "    emb_dists, emb_indices = neigh_emb.kneighbors(X_emb)\n",
    "\n",
    "    npres = np.zeros(Khigh)\n",
    "    \n",
    "    for k in range(1, Khigh+1):\n",
    "        for i in range(X.shape[0]):\n",
    "            high_current = high_indices[i][1:k+1]\n",
    "            emb_current = emb_indices[i][1:k+1]\n",
    "            \n",
    "            tp = len(np.intersect1d(high_current, emb_current))\n",
    "            \n",
    "            npres[k-1] += (tp/k)\n",
    "        \n",
    "        \n",
    "    npres /= float(X.shape[0])\n",
    "    \n",
    "    return npres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fmnist():\n",
    "    fashionTrain = pd.read_csv('data/fashion-train.csv')\n",
    "\n",
    "    fashionX = fashionTrain.values[:,2:]\n",
    "    fashionY = fashionTrain.values[:, 1].astype(int)\n",
    "\n",
    "    X = normalize(fashionX)\n",
    "    y = fashionY\n",
    "\n",
    "    X = check_array(X, dtype=np.float32, accept_sparse='csr', order='C')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def get_mnist():\n",
    "    X = np.load('./data/MNIST_70000.npy')\n",
    "    y = np.load('./data/MNIST_70000_label.npy').astype(int)\n",
    "    X = normalize(X)\n",
    "    X = check_array(X, dtype=np.float32, accept_sparse='csr', order='C')\n",
    "    \n",
    "    return X, y\n",
    "X, y = get_fmnist()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672 parameters.\n"
     ]
    }
   ],
   "source": [
    "distance_similarities = [False, True]\n",
    "path_increments = [False, True]\n",
    "n_neighbors =  [15, 20, 40, 50, 70, 90, 100]\n",
    "landmarks_nwalks = [10, 20, 30]\n",
    "landmarks_walklengths = [5, 10, 20, 30, 50, 100]\n",
    "influence_nwalks =  [10, 20, 30]\n",
    "influence_walklengths = [10, 30, 50, 70, 80, 90, 100]\n",
    "influence_neighborhoods = [0.0, 0.1, 0.2, 0.3, 0.5, 0.7, 0.9, 1]\n",
    "min_dists = [0.05,0.1, 0.15]\n",
    "executions = 10\n",
    "\n",
    "n_parameters = len(distance_similarities) * len(path_increments) * len(n_neighbors) * len(landmarks_nwalks) * len(influence_neighborhoods) \n",
    "print(\"%d parameters.\" % (n_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "id_conf = 0\n",
    "file_conf = open('experiments/humap_parameters/current_analysis.csv', 'w')\n",
    "file_conf.write('id,distance_similarity,path_increment,n_neighbors,landmarks_nwlandmarks_wl,influence_nw,influence_wl,influence_neighborhood,n_dist\\n')\n",
    "\n",
    "for distance_similarity in distance_similarities:\n",
    "    for path_increment in path_increments:\n",
    "        for n_neigh in n_neighbors:\n",
    "            for landmarks_nwalk in landmarks_nwalks:\n",
    "                for landmarks_walklength in landmarks_walklengths:\n",
    "                    for influence_nwalk in influence_nwalks:\n",
    "                        for influence_walklength in influence_walklengths:\n",
    "                            for influence_neighborhood in influence_neighborhoods:\n",
    "                                for min_dist in min_dists:\n",
    "\n",
    "                                    inf_neighborhood = int(influence_neighborhood*n_neigh)\n",
    "                                    path = 'experiments/humap_parameters/'+str(id_conf)+'/'\n",
    "                                    id_conf += 1\n",
    "                                    os.mkdir(path)\n",
    "                                    \n",
    "                                    correlation_list = []\n",
    "                                    np_list = []\n",
    "                                    demap_list = []\n",
    "                                    \n",
    "                                    for i in range(executions):\n",
    "\n",
    "                                        reducer = h_umap.HUMAP('precomputed', np.array([0.20, 0.19]), n_neigh, min_dist, 'NNDescent', 0.0, True)\n",
    "\n",
    "                                        reducer.set_distance_similarity(distance_similarity)\n",
    "                                        reducer.set_path_increment(path_increment)\n",
    "                                        reducer.set_landmarks_nwalks(landmarks_nwalk)\n",
    "                                        reducer.set_landmarks_wl(landmarks_walklength)\n",
    "                                        reducer.set_influence_nwalks(influence_nwalk)\n",
    "                                        reducer.set_influence_wl(influence_walklength)\n",
    "                                        reducer.set_influence_neighborhood(inf_neighborhood)\n",
    "\n",
    "                                        reducer.fit(X, y)\n",
    "                                        embedding_2 = reducer.get_embedding(2)\n",
    "                                        y_2 = reducer.get_labels(2)\n",
    "                                        X_2 = X[reducer.get_original_indices(2), :]\n",
    "                                        \n",
    "                                        indices_2 = random.sample(range(0, len(y_2)), min(3000, len(y_2)))\n",
    "                                        subset_emb_2 = embedding_2[indices_2]\n",
    "                                        subset_X_2 = X_2[indices_2]\n",
    "                                        \n",
    "                                        plt.scatter(embedding_2[:, 0], embedding_2[:, 1], c=y_2, alpha=0.6, cmap='Spectral')\n",
    "                                        plt.savefig(path+'/embedding_'+str(i)+'.svg')\n",
    "                                        plt.clf()\n",
    "                                        \n",
    "                                        demap_value = demap.DEMaP(subset_X_2, subset_emb_2)\n",
    "                                        demap_list.append(demap_value)\n",
    "                                        corr_value = correlation(subset_X_2, subset_emb_2)\n",
    "                                        correlation_list.append(corr_value)\n",
    "                                        npres = neighborhood_preservation(subset_X_2, subset_emb_2)\n",
    "                                        np_list = np_list + npres.tolist()\n",
    "                                    \n",
    "                                    df_corr = pd.DataFrame({\n",
    "                                        'index': list(range(len(correlation_list))),\n",
    "                                        'corr': correlation_list\n",
    "                                    })\n",
    "                                    \n",
    "                                    df_np = pd.DataFrame({\n",
    "                                        'n_neighbors': list(range(30))*executions,\n",
    "                                        'n_preservation': np_list\n",
    "                                    })\n",
    "                                    \n",
    "                                    df_demap = pd.DataFrame({\n",
    "                                        'index': list(range(len(demap_list))),\n",
    "                                        'demap': demap_list\n",
    "                                    })\n",
    "                                    \n",
    "                                    df_corr.to_csv(path+'/correlation.csv', index=False)\n",
    "                                    df_np.to_csv(path+'/npreservation.csv', index=False)\n",
    "                                    df_demap.to_csv(path+'/demap.csv', index=False)\n",
    "                                    \n",
    "\n",
    "                                    file_conf.write(str(id_conf)+','+str(distance_similarity)+','+str(path_increment)+','+str(n_neigh)+','+str(landmarks_nwalk)+','+str(landmarks_walklength)+','+str(influence_nwalk)+','+str(influence_walklength)+','+str(influence_neighborhood)+','+str(min_dist)+'\\n')\n",
    "                                    \n",
    "                                        \n",
    "                                        \n",
    "                                        \n",
    "                                        \n",
    "file_conf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_similarity = \n",
    "path_increment = \n",
    "n_neighbors = \n",
    "min_dist = \n",
    "\n",
    "landmarks_nwalks = [5, 10, 20, 30, 50, 100]\n",
    "landmarks_walklengths = [5, 10, 20, 30, 50, 100]\n",
    "influence_nwalks = [5, 10, 20, 30, 50, 100]\n",
    "influence_walklengths = [5, 10, 20, 30, 50, 60, 70, 80, 90, 100]\n",
    "influence_neighborhoods = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for landmarks_nwalk in landmarks_nwalks:\n",
    "    for landmarks_walklength in landmarks_walklengths:\n",
    "        for influence_nwalk in influence_nwalks:\n",
    "            for influence_walklength in influence_walklengths:\n",
    "                for influence_neighborhood in influence_neighborhoods:\n",
    "\n",
    "                    inf_neighborhood = int(influence_neighborhood*n_neigh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
