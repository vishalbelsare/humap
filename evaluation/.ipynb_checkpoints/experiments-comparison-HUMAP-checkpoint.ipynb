{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nptsne\n",
    "from nptsne import hsne_analysis\n",
    "import multiscale_phate as mp\n",
    "\n",
    "import time\n",
    "import os\n",
    "import scprep\n",
    "import demap\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hierarchical_umap as h_umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import normalize, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fmnist():\n",
    "    fashionTrain = pd.read_csv('data/fashion-train.csv')\n",
    "\n",
    "    fashionX = fashionTrain.values[:,2:]\n",
    "    fashionY = fashionTrain.values[:, 1].astype(int)\n",
    "\n",
    "    X = normalize(fashionX)\n",
    "    y = fashionY\n",
    "\n",
    "    X = check_array(X, dtype=np.float32, accept_sparse='csr', order='C')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def load_mnist():\n",
    "    X = np.load('./data/MNIST_70000.npy')\n",
    "    y = np.load('./data/MNIST_70000_label.npy').astype(int)\n",
    "    X = normalize(X)\n",
    "    X = check_array(X, dtype=np.float32, accept_sparse='csr', order='C')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def load_scRNAseq():\n",
    "    download_path = os.path.expanduser(\"~/Documentos/HierarchicalUMAP/umap-cpp/umap/cpp/data\")\n",
    "    sparse=True\n",
    "    T1 = scprep.io.load_10X(os.path.join(download_path, \"scRNAseq\", \"T0_1A\"), sparse=sparse, gene_labels='both')\n",
    "    T2 = scprep.io.load_10X(os.path.join(download_path, \"scRNAseq\", \"T2_3B\"), sparse=sparse, gene_labels='both')\n",
    "    T3 = scprep.io.load_10X(os.path.join(download_path, \"scRNAseq\", \"T4_5C\"), sparse=sparse, gene_labels='both')\n",
    "    T4 = scprep.io.load_10X(os.path.join(download_path, \"scRNAseq\", \"T6_7D\"), sparse=sparse, gene_labels='both')\n",
    "    T5 = scprep.io.load_10X(os.path.join(download_path, \"scRNAseq\", \"T8_9E\"), sparse=sparse, gene_labels='both')\n",
    "    filtered_batches = []\n",
    "    for batch in [T1, T2, T3, T4, T5]:\n",
    "        batch = scprep.filter.filter_library_size(batch, percentile=20, keep_cells='above')\n",
    "        batch = scprep.filter.filter_library_size(batch, percentile=75, keep_cells='below')\n",
    "        filtered_batches.append(batch)\n",
    "    del T1, T2, T3, T4, T5\n",
    "    EBT_counts, sample_labels = scprep.utils.combine_batches(\n",
    "        filtered_batches, \n",
    "        [\"Day 00-03\", \"Day 06-09\", \"Day 12-15\", \"Day 18-21\", \"Day 24-27\"],\n",
    "        append_to_cell_names=True\n",
    "    )\n",
    "    del filtered_batches # removes objects from memory\n",
    "    EBT_counts = scprep.filter.filter_rare_genes(EBT_counts, min_cells=10)\n",
    "    EBT_counts = scprep.normalize.library_size_normalize(EBT_counts)\n",
    "    mito_genes = scprep.select.get_gene_set(EBT_counts, starts_with=\"MT-\") # Get all mitochondrial genes. There are 14, FYI.\n",
    "    EBT_counts, sample_labels = scprep.filter.filter_gene_set_expression(\n",
    "    EBT_counts, sample_labels, genes=mito_genes, \n",
    "    percentile=90, keep_cells='below')\n",
    "    EBT_counts = scprep.transform.sqrt(EBT_counts)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(sample_labels)\n",
    "    labels = le.transform(sample_labels)\n",
    "    X = PCA(n_components=50).fit_transform(EBT_counts.values)\n",
    "    X = check_array(X, dtype=np.float32, accept_sparse='csr', order='C')\n",
    "    return X, labels\n",
    "\n",
    "def load_mammals():\n",
    "    X = np.loadtxt(\"data/mammals-20000_features.txt\")\n",
    "    y = np.loadtxt(\"data/mammals-20000_classes.txt\")\n",
    "    X = normalize(X)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scRNAseq dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [19:47<00:00, 59.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mammals dataset...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [24:15<00:00, 72.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fmnist dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [1:29:28<00:00, 268.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mnist dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [1:52:13<00:00, 336.70s/it]\n"
     ]
    }
   ],
   "source": [
    "n_executions = 20\n",
    "levels = 3\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "df_humapFLANN_level2 = pd.DataFrame()\n",
    "df_humapFLANN_level0 = pd.DataFrame()\n",
    "\n",
    "df_humapKDTREE_NN_level2 = pd.DataFrame()\n",
    "df_humapKDTREE_NN_level0 = pd.DataFrame()\n",
    "\n",
    "datasets = []\n",
    "\n",
    "datasets.append({\n",
    "   'load': load_scRNAseq,\n",
    "   'name': 'scRNAseq'\n",
    "})\n",
    "datasets.append({\n",
    "   'load': load_mammals,\n",
    "   'name': 'mammals'\n",
    "})\n",
    "datasets.append({\n",
    "    'load': load_fmnist,\n",
    "    'name': 'fmnist'\n",
    "})\n",
    "datasets.append({\n",
    "    'load': load_mnist,\n",
    "    'name': 'mnist'\n",
    "})\n",
    "    \n",
    "    \n",
    "for dataset in datasets:\n",
    "    print(\"Loading %s dataset...\" % (dataset['name']))\n",
    "    X, y = dataset['load']()\n",
    "    print(\"Done.\")\n",
    "    init = 0\n",
    "\n",
    "    hsneGPU = nptsne.HSne(True)\n",
    "    hsneGPU.create_hsne(X, 3)\n",
    "\n",
    "    n_level0 = hsneGPU.get_scale(0).num_points\n",
    "    n_level1 = hsneGPU.get_scale(1).num_points\n",
    "    n_level2 = hsneGPU.get_scale(2).num_points\n",
    "    \n",
    "    for execution in tqdm(range(init, n_executions)):\n",
    "        \n",
    "        time_file = open(\"experiments/comparison/\"+dataset['name']+'/run-time.csv', 'a')\n",
    "        size_file = open(\"experiments/comparison/\"+dataset['name']+'/mphate_size_level2.csv', 'a')\n",
    "\n",
    "        \n",
    "\n",
    "        hUmapFLANN = h_umap.HUMAP('precomputed', np.array([n_level1/n_level0, n_level2/n_level2]), 100, 0.15, \"FLANN\", 0.0, True)\n",
    "        hUmapFLANN.set_distance_similarity(False)\n",
    "        hUmapFLANN.set_path_increment(False)\n",
    "        hUmapFLANN.set_influence_neighborhood(0)\n",
    "        \n",
    "\n",
    "        tic = time.time()\n",
    "\n",
    "        hUmapFLANN.fit(X, y)\n",
    "        embedding2 = hUmapFLANN.transform(2)\n",
    "        embedding0 = hUmapFLANN.transform(0)\n",
    "        execution_humapFLANN = time.time() - tic\n",
    "\n",
    "        df_humapFLANN_level2['label'+str(execution)] = hUmapFLANN.get_labels(2)\n",
    "        df_humapFLANN_level2['x'+str(execution)] = embedding2[:, 0]\n",
    "        df_humapFLANN_level2['y'+str(execution)] = embedding2[:, 1]\n",
    "        df_humapFLANN_level2['inds'+str(execution)] = hUmapFLANN.get_original_indices(2)\n",
    "\n",
    "        df_humapFLANN_level0['label'+str(execution)] = y\n",
    "        df_humapFLANN_level0['x'+str(execution)] = embedding0[:, 0]\n",
    "        df_humapFLANN_level0['y'+str(execution)] = embedding0[:, 1]\n",
    "        df_humapFLANN_level0['inds'+str(execution)] = np.arange(len(y))\n",
    "        \n",
    "        \n",
    "        hUmapKDTREE_NN = h_umap.HUMAP('precomputed', np.array([n_level1/n_level0, n_level2/n_level2]), 100, 0.15, \"KDTree_NNDescent\", 0.0, True)\n",
    "        hUmapKDTREE_NN.set_distance_similarity(False)\n",
    "        hUmapKDTREE_NN.set_path_increment(False)\n",
    "        hUmapKDTREE_NN.set_influence_neighborhood(0)\n",
    "        \n",
    "\n",
    "        tic = time.time()\n",
    "\n",
    "        hUmapKDTREE_NN.fit(X, y)\n",
    "        embedding2 = hUmapKDTREE_NN.transform(2)\n",
    "        embedding0 = hUmapKDTREE_NN.transform(0)\n",
    "        execution_humapKDTREE_NN = time.time() - tic\n",
    "\n",
    "        df_humapKDTREE_NN_level2['label'+str(execution)] = hUmapKDTREE_NN.get_labels(2)\n",
    "        df_humapKDTREE_NN_level2['x'+str(execution)] = embedding2[:, 0]\n",
    "        df_humapKDTREE_NN_level2['y'+str(execution)] = embedding2[:, 1]\n",
    "        df_humapKDTREE_NN_level2['inds'+str(execution)] = hUmapKDTREE_NN.get_original_indices(2)\n",
    "\n",
    "        df_humapKDTREE_NN_level0['label'+str(execution)] = y\n",
    "        df_humapKDTREE_NN_level0['x'+str(execution)] = embedding0[:, 0]\n",
    "        df_humapKDTREE_NN_level0['y'+str(execution)] = embedding0[:, 1]\n",
    "        df_humapKDTREE_NN_level0['inds'+str(execution)] = np.arange(len(y))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "        df_humapFLANN_level2.to_csv(\"experiments/comparison/\"+dataset['name']+'/humapFLANN_it'+str(execution)+'_level2.csv', index=False)\n",
    "        df_humapFLANN_level0.to_csv(\"experiments/comparison/\"+dataset['name']+'/humapFLANN_it'+str(execution)+'_level0.csv', index=False)\n",
    "        \n",
    "        df_humapKDTREE_NN_level2.to_csv(\"experiments/comparison/\"+dataset['name']+'/humapKDTREE_NN_it'+str(execution)+'_level2.csv', index=False)\n",
    "        df_humapKDTREE_NN_level0.to_csv(\"experiments/comparison/\"+dataset['name']+'/humapKDTREE_NN_it'+str(execution)+'_level0.csv', index=False)\n",
    "        \n",
    "        df_humapFLANN_level2 = pd.DataFrame()\n",
    "        df_humapFLANN_level0 = pd.DataFrame()\n",
    "        \n",
    "        df_humapKDTREE_NN_level2 = pd.DataFrame()\n",
    "        df_humapKDTREE_NN_level0 = pd.DataFrame()\n",
    "        \n",
    "        time_file.write('HUMAP FLANN,'+str(execution_humapFLANN)+'\\n')\n",
    "        time_file.write('HUMAP KDTree + NNDescent,'+str(execution_humapKDTREE_NN)+'\\n')\n",
    "        \n",
    "        \n",
    "            \n",
    "#     df_times = pd.DataFrame({\n",
    "#         'HSNE CPU': hsneCPU_time,\n",
    "#         'HSNE GPU': hsneGPU_time,\n",
    "#         'HUMAP': humap_time,\n",
    "#         'Multiscale PHATE': mphate_time\n",
    "#     })\n",
    "    \n",
    "#     df_sizes = pd.DataFrame({\n",
    "#         'Size': mphate_sizes\n",
    "#     })\n",
    "    \n",
    "    \n",
    "#     df_times.to_csv(\"experiments/comparison/\"+dataset['name']+'/time_execution.csv', index=False)\n",
    "#     df_sizes.to_csv(\"experiments/comparison/\"+dataset['name']+'/mphate_size_level2.csv', index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testar HUMAP com FAISS, KDtree e FLANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_hsneCPU_level0['x0'].values, df_hsneCPU_level0['y0'].values, c=df_hsneCPU_level0['label0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_hsneGPU_level2['x0'].values, df_hsneGPU_level2['y0'].values, c=df_hsneGPU_level2['label0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_hsneGPU_level0['x0'].values, df_hsneGPU_level0['y0'].values, c=df_hsneGPU_level0['label0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_humap_level2['x0'].values, df_humap_level2['y0'].values, c=df_humap_level2['label0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_humap_level0['x0'].values, df_humap_level0['y0'].values, c=df_humap_level0['label0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_mphate_level2['x0'].values, df_mphate_level2['y0'].values, c=df_mphate_level2['label0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_mphate_level0['x0'].values, df_mphate_level0['y0'].values, c=df_mphate_level0['label0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
