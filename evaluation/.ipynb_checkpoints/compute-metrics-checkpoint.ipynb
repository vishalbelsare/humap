{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scprep\n",
    "import demap\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import humap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import normalize, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(X, X_emb):\n",
    "    \n",
    "    dist_orig = np.square(euclidean_distances(X, X)).flatten()\n",
    "    dist_emb = np.square(euclidean_distances(X_emb, X_emb)).flatten()\n",
    "    \n",
    "    \n",
    "    coef, p = spearmanr(dist_orig, dist_emb)\n",
    "    return coef\n",
    "\n",
    "def stress(X, X_emb):\n",
    "    \n",
    "    DE = euclidean_distances(X_emb)\n",
    "    DE = DE/np.max(DE)\n",
    "    DH = euclidean_distances(X)\n",
    "    DH = DH/np.max(DH)\n",
    "    stress = 0.5 * np.sum((DE - DH)**2)\n",
    "    \n",
    "    return np.sqrt(stress/(0.5*np.sum(DH**2)))\n",
    "    \n",
    "\n",
    "def neighborhood_preservation(X, X_emb, Khigh=30):\n",
    "    \n",
    "    neigh_high = NearestNeighbors(n_neighbors=Khigh+1, n_jobs=-1)\n",
    "    neigh_high.fit(X)\n",
    "    high_dists, high_indices = neigh_high.kneighbors(X)\n",
    "\n",
    "\n",
    "    neigh_emb = NearestNeighbors(n_neighbors=Khigh+1, n_jobs=-1)\n",
    "    neigh_emb.fit(X_emb)\n",
    "    emb_dists, emb_indices = neigh_emb.kneighbors(X_emb)\n",
    "\n",
    "    npres = np.zeros(Khigh)\n",
    "    \n",
    "    for k in range(1, Khigh+1):\n",
    "        for i in range(X.shape[0]):\n",
    "            high_current = high_indices[i][1:k+1]\n",
    "            emb_current = emb_indices[i][1:k+1]\n",
    "            \n",
    "            tp = len(np.intersect1d(high_current, emb_current))\n",
    "            \n",
    "            npres[k-1] += (tp/k)\n",
    "        \n",
    "        \n",
    "    npres /= float(X.shape[0])\n",
    "    \n",
    "    return npres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fmnist():\n",
    "    fashionTrain = pd.read_csv('./../data/fashion-train.csv')\n",
    "\n",
    "    fashionX = fashionTrain.values[:,2:]\n",
    "    fashionY = fashionTrain.values[:, 1].astype(int)\n",
    "\n",
    "    X = normalize(fashionX)\n",
    "    y = fashionY\n",
    "\n",
    "    X = check_array(X, dtype=np.float32, accept_sparse='csr', order='C')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def load_mnist():\n",
    "    X = np.load('./../data/MNIST_70000.npy')\n",
    "    y = np.load('./../data/MNIST_70000_label.npy').astype(int)\n",
    "    X = normalize(X)\n",
    "    X = check_array(X, dtype=np.float32, accept_sparse='csr', order='C')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def load_scRNAseq():\n",
    "    download_path = os.path.expanduser(\"./../data\")\n",
    "    sparse=True\n",
    "    T1 = scprep.io.load_10X(os.path.join(download_path, \"scRNAseq\", \"T0_1A\"), sparse=sparse, gene_labels='both')\n",
    "    T2 = scprep.io.load_10X(os.path.join(download_path, \"scRNAseq\", \"T2_3B\"), sparse=sparse, gene_labels='both')\n",
    "    T3 = scprep.io.load_10X(os.path.join(download_path, \"scRNAseq\", \"T4_5C\"), sparse=sparse, gene_labels='both')\n",
    "    T4 = scprep.io.load_10X(os.path.join(download_path, \"scRNAseq\", \"T6_7D\"), sparse=sparse, gene_labels='both')\n",
    "    T5 = scprep.io.load_10X(os.path.join(download_path, \"scRNAseq\", \"T8_9E\"), sparse=sparse, gene_labels='both')\n",
    "    filtered_batches = []\n",
    "    for batch in [T1, T2, T3, T4, T5]:\n",
    "        batch = scprep.filter.filter_library_size(batch, percentile=20, keep_cells='above')\n",
    "        batch = scprep.filter.filter_library_size(batch, percentile=75, keep_cells='below')\n",
    "        filtered_batches.append(batch)\n",
    "    del T1, T2, T3, T4, T5\n",
    "    EBT_counts, sample_labels = scprep.utils.combine_batches(\n",
    "        filtered_batches, \n",
    "        [\"Day 00-03\", \"Day 06-09\", \"Day 12-15\", \"Day 18-21\", \"Day 24-27\"],\n",
    "        append_to_cell_names=True\n",
    "    )\n",
    "    del filtered_batches # removes objects from memory\n",
    "    EBT_counts = scprep.filter.filter_rare_genes(EBT_counts, min_cells=10)\n",
    "    EBT_counts = scprep.normalize.library_size_normalize(EBT_counts)\n",
    "    mito_genes = scprep.select.get_gene_set(EBT_counts, starts_with=\"MT-\") # Get all mitochondrial genes. There are 14, FYI.\n",
    "    EBT_counts, sample_labels = scprep.filter.filter_gene_set_expression(\n",
    "    EBT_counts, sample_labels, genes=mito_genes, \n",
    "    percentile=90, keep_cells='below')\n",
    "    EBT_counts = scprep.transform.sqrt(EBT_counts)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(sample_labels)\n",
    "    labels = le.transform(sample_labels)\n",
    "    X = PCA(n_components=50).fit_transform(EBT_counts.values)\n",
    "    X = check_array(X, dtype=np.float32, accept_sparse='csr', order='C')\n",
    "    return X, labels\n",
    "\n",
    "def load_mammals():\n",
    "    X = np.loadtxt(\"./../data/mammals-20000_features.txt\")\n",
    "    y = np.loadtxt(\"./../data/mammals-20000_classes.txt\")\n",
    "    X = normalize(X)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "datasets.append({\n",
    "    'load': load_mammals,\n",
    "    'name': 'mammals'\n",
    "})\n",
    "datasets.append({\n",
    "    'load': load_scRNAseq,\n",
    "    'name': 'scRNAseq'\n",
    "})\n",
    "datasets.append({\n",
    "    'load': load_fmnist,\n",
    "    'name': 'fmnist'\n",
    "})\n",
    "datasets.append({\n",
    "    'load': load_mnist,\n",
    "    'name': 'mnist'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(datasets, techniques = ['hsneCPU', 'hsneGPU', 'mphate', 'humap'], n_executions = 20):\n",
    "    map_name = {}\n",
    "    map_name['hsneCPU'] = 'HSNE CPU'\n",
    "    map_name['hsneGPU'] = 'HSNE GPU'\n",
    "    map_name['mphate'] = 'Multiscale PHATE'\n",
    "    map_name['humap'] = 'HUMAP'\n",
    "    \n",
    "    \n",
    "    corr_values = []\n",
    "    corr_level = []\n",
    "    corr_technique = []\n",
    "    \n",
    "    demap_values = []\n",
    "    demap_level = []\n",
    "    demap_technique = []\n",
    "    \n",
    "    neighborhood_values = []\n",
    "    np_values = []\n",
    "    np_level = []\n",
    "    np_technique = []\n",
    "    \n",
    "    dataset_values_corr = []\n",
    "    dataset_values_demap = []\n",
    "    dataset_values_np = []\n",
    "    \n",
    "    size_before_corr = 0\n",
    "    size_before_demap = 0\n",
    "    size_before_np = 0\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        \n",
    "        path = 'comparison-techniques/'+dataset['name']\n",
    "        X, y = dataset['load']()\n",
    "        \n",
    "        print(\"DATASET: %s\" % (dataset['name']))\n",
    "        \n",
    "                \n",
    "        for technique in techniques:\n",
    "            print(\"Technique: %s\" % (technique))\n",
    "            \n",
    "            if (dataset['name'] == 'fmnist' or dataset['name'] == 'mnist') and technique == 'mphate':\n",
    "                continue\n",
    "                \n",
    "            for i, level in enumerate(['_level2.csv', '_level1.csv', '_level0.csv']):\n",
    "                \n",
    "                print(\"Level: %s\" % (level))\n",
    "                \n",
    "                demap_list = []\n",
    "                correlation_list = []\n",
    "                np_list = []\n",
    "\n",
    "                for execution in tqdm(range(n_executions)):\n",
    "                    it = str(execution)\n",
    "                    \n",
    "                    df = pd.read_csv(path+'/'+technique+'_it'+it+level)\n",
    "                    x = df['x'+it].values\n",
    "                    y = df['y'+it].values\n",
    "                    indices = df['inds'+it].values\n",
    "                    \n",
    "                    emb = np.stack((x, y), axis=-1)\n",
    "                    sample = random.sample(range(0, len(x)), min(3000, len(x)))\n",
    "                    \n",
    "                    subset_emb = emb[sample]\n",
    "                    subset_X = X[indices][sample]\n",
    "                    \n",
    "                    demap_value = demap.DEMaP(subset_X, subset_emb)\n",
    "                    demap_list.append(demap_value)\n",
    "                    \n",
    "                    corr_value = correlation(subset_X, subset_emb)\n",
    "                    correlation_list.append(corr_value)\n",
    "                    \n",
    "                    npres_values = neighborhood_preservation(subset_X, subset_emb)\n",
    "                    np_list = np_list + npres_values.tolist()\n",
    "                    \n",
    "                    \n",
    "                corr_values = corr_values + correlation_list\n",
    "                corr_technique = corr_technique + [map_name[technique]]*len(correlation_list)\n",
    "                level = 0\n",
    "                if i == 0:\n",
    "                    level = 2\n",
    "                elif i == 1:\n",
    "                    level = 1\n",
    "    \n",
    "                corr_level = corr_level + ['Level '+str(level)]*len(correlation_list)\n",
    "                \n",
    "                demap_values = demap_values + demap_list\n",
    "                demap_technique = demap_technique + [map_name[technique]]*len(demap_list)\n",
    "                demap_level = demap_level + ['Level '+str(level)]*len(demap_list)\n",
    "                \n",
    "                np_values = np_values + np_list\n",
    "                neighborhood_values = neighborhood_values + list(range(30))*n_executions\n",
    "                np_technique = np_technique + [map_name[technique]]*len(np_list)\n",
    "                np_level = np_level + ['Level '+str(level)]*len(np_list)\n",
    "        \n",
    "        \n",
    "        dataset_values_corr = dataset_values_corr + [dataset['name']]*(len(corr_values) - size_before_corr)\n",
    "        dataset_values_demap = dataset_values_demap + [dataset['name']]*(len(demap_values) - size_before_demap)\n",
    "        dataset_values_np = dataset_values_np + [dataset['name']]*(len(np_values) - size_before_np)\n",
    "        \n",
    "        size_before_corr = len(corr_values)\n",
    "        size_before_demap = len(demap_values)\n",
    "        size_before_np = len(np_values)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    print(len(dataset_values_corr), len(corr_technique), len(corr_values))\n",
    "    \n",
    "    df_correlation = pd.DataFrame({\n",
    "        'dataset': dataset_values_corr,\n",
    "        'technique': corr_technique,\n",
    "        'level': corr_level,\n",
    "        'values': corr_values        \n",
    "    })\n",
    "    \n",
    "    print(len(dataset_values_demap), len(demap_technique), len(demap_values))\n",
    "    \n",
    "    df_demap = pd.DataFrame({\n",
    "        'dataset': dataset_values_demap,\n",
    "        'technique': demap_technique,\n",
    "        'level': demap_level,\n",
    "        'values': demap_values\n",
    "    })\n",
    "    \n",
    "    print(len(dataset_values_np), len(np_technique), len(neighborhood_values), len(np_values))\n",
    "    \n",
    "    \n",
    "    df_np = pd.DataFrame({\n",
    "        'dataset': dataset_values_np,\n",
    "        'technique': np_technique,\n",
    "        'level': np_level,\n",
    "        'neighbors': neighborhood_values,\n",
    "        'np': np_values\n",
    "    })\n",
    "        \n",
    "    return df_correlation, df_demap, df_np            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET: mammals\n",
      "Technique: hsneCPU\n",
      "Level: _level2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: _level1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:28<00:00,  7.45s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: _level0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:33<00:00,  7.68s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technique: hsneGPU\n",
      "Level: _level2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: _level1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:28<00:00,  7.42s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: _level0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:29<00:00,  7.48s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technique: mphate\n",
      "Level: _level2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:24<00:00,  1.24s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: _level1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:24<00:00,  1.24s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: _level0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:27<00:00,  7.36s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technique: humap\n",
      "Level: _level2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.22it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: _level1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:27<00:00,  7.38s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: _level0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:29<00:00,  7.50s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET: scRNAseq\n",
      "Technique: hsneCPU\n",
      "Level: _level2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:19<00:00,  1.01it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: _level1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:45<00:00, 11.28s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: _level0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:50<00:00, 11.51s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technique: hsneGPU\n",
      "Level: _level2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:19<00:00,  1.01it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: _level1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:43<00:00, 11.17s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: _level0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:55<00:00, 11.75s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technique: mphate\n",
      "Level: _level2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:32<00:00, 10.60s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: _level1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:56<00:00, 11.83s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: _level0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:54<00:00, 11.71s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technique: humap\n",
      "Level: _level2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:20<00:00,  1.01s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: _level1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:46<00:00, 11.30s/it]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: _level0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-60d7c507882a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_correlation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_demap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-6de9d7316faa>\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(datasets, techniques, n_executions)\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0msubset_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                     \u001b[0mdemap_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdemap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEMaP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                     \u001b[0mdemap_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemap_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/demap/demap.py\u001b[0m in \u001b[0;36mDEMaP\u001b[0;34m(data, embedding, knn, subsample_idx)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mDEMaP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgeodesic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeodesic_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mknn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubsample_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mgeodesic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeodesic_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubsample_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/demap/demap.py\u001b[0m in \u001b[0;36mgeodesic_distance\u001b[0;34m(data, knn, distance)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgeodesic_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mknn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortest_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/graphtools/base.py\u001b[0m in \u001b[0;36mshortest_path\u001b[0;34m(self, method, distance)\u001b[0m\n\u001b[1;32m    904\u001b[0m             )\n\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_shortest_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m         \u001b[0;31m# symmetrize for numerical error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_correlation, df_demap, df_np = compute_metrics(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correlation.to_csv('./correlation_values.csv', index=False)\n",
    "df_demap.to_csv('./demap_values.csv', index=False)\n",
    "df_np.to_csv('./np_values.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
