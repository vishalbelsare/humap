{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nptsne\n",
    "from nptsne import hsne_analysis\n",
    "import multiscale_phate as mp\n",
    "\n",
    "import time\n",
    "import os\n",
    "import scprep\n",
    "import demap\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hierarchical_umap as h_umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import normalize, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fmnist():\n",
    "    fashionTrain = pd.read_csv('data/fashion-train.csv')\n",
    "\n",
    "    fashionX = fashionTrain.values[:,2:]\n",
    "    fashionY = fashionTrain.values[:, 1].astype(int)\n",
    "\n",
    "    X = normalize(fashionX)\n",
    "    y = fashionY\n",
    "#     X = PCA(n_components=15).fit_transform(X)\n",
    "    X = check_array(X, dtype=np.float32, accept_sparse='csr', order='C')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def load_mnist():\n",
    "    X = np.load('./data/MNIST_70000.npy')\n",
    "    y = np.load('./data/MNIST_70000_label.npy').astype(int)\n",
    "    X = normalize(X)\n",
    "#     X = PCA(n_components=15).fit_transform(X)\n",
    "    X = check_array(X, dtype=np.float32, accept_sparse='csr', order='C')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def load_scRNAseq():\n",
    "    download_path = os.path.expanduser(\"~/Documentos/HierarchicalUMAP/umap-cpp/umap/cpp/data\")\n",
    "    sparse=True\n",
    "    T1 = scprep.io.load_10X(os.path.join(download_path, \"scRNAseq\", \"T0_1A\"), sparse=sparse, gene_labels='both')\n",
    "    T2 = scprep.io.load_10X(os.path.join(download_path, \"scRNAseq\", \"T2_3B\"), sparse=sparse, gene_labels='both')\n",
    "    T3 = scprep.io.load_10X(os.path.join(download_path, \"scRNAseq\", \"T4_5C\"), sparse=sparse, gene_labels='both')\n",
    "    T4 = scprep.io.load_10X(os.path.join(download_path, \"scRNAseq\", \"T6_7D\"), sparse=sparse, gene_labels='both')\n",
    "    T5 = scprep.io.load_10X(os.path.join(download_path, \"scRNAseq\", \"T8_9E\"), sparse=sparse, gene_labels='both')\n",
    "    filtered_batches = []\n",
    "    for batch in [T1, T2, T3, T4, T5]:\n",
    "        batch = scprep.filter.filter_library_size(batch, percentile=20, keep_cells='above')\n",
    "        batch = scprep.filter.filter_library_size(batch, percentile=75, keep_cells='below')\n",
    "        filtered_batches.append(batch)\n",
    "    del T1, T2, T3, T4, T5\n",
    "    EBT_counts, sample_labels = scprep.utils.combine_batches(\n",
    "        filtered_batches, \n",
    "        [\"Day 00-03\", \"Day 06-09\", \"Day 12-15\", \"Day 18-21\", \"Day 24-27\"],\n",
    "        append_to_cell_names=True\n",
    "    )\n",
    "    del filtered_batches # removes objects from memory\n",
    "    EBT_counts = scprep.filter.filter_rare_genes(EBT_counts, min_cells=10)\n",
    "    EBT_counts = scprep.normalize.library_size_normalize(EBT_counts)\n",
    "    mito_genes = scprep.select.get_gene_set(EBT_counts, starts_with=\"MT-\") # Get all mitochondrial genes. There are 14, FYI.\n",
    "    EBT_counts, sample_labels = scprep.filter.filter_gene_set_expression(\n",
    "    EBT_counts, sample_labels, genes=mito_genes, \n",
    "    percentile=90, keep_cells='below')\n",
    "    EBT_counts = scprep.transform.sqrt(EBT_counts)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(sample_labels)\n",
    "    labels = le.transform(sample_labels)\n",
    "    X = PCA(n_components=50).fit_transform(EBT_counts.values)\n",
    "    X = check_array(X, dtype=np.float32, accept_sparse='csr', order='C')\n",
    "    return X, labels\n",
    "\n",
    "def load_mammals():\n",
    "    X = np.loadtxt(\"data/mammals-20000_features.txt\")\n",
    "    y = np.loadtxt(\"data/mammals-20000_classes.txt\")\n",
    "    X = normalize(X)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mammals dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "initing with 2\n",
      "Initing at 2\n",
      "Calculating Multiscale PHATE tree...\n",
      "  Calculating PCA...\n",
      "  Calculated PCA in 0.25 seconds.\n",
      "  Calculating diffusion potential...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wilson/anaconda3/envs/py37/lib/python3.7/site-packages/graphtools/graphs.py:293: RuntimeWarning: Detected zero distance between 3510 pairs of samples. Consider removing duplicates to avoid errors in downstream processing.\n",
      "  RuntimeWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Calculated diffusion potential in 19.35 seconds.\n",
      "  Setting epsilon to 7.862\n",
      "  Setting merge threshold to 0.001\n",
      "  Calculating condensation...\n",
      "  Calculated condensation in 40.68 seconds.\n",
      "Calculated Multiscale PHATE tree in 61.75 seconds.\n",
      "Computing gradient...\n",
      "Identifying salient levels of resolution...\n",
      "level: 1, n: 922, dif: 268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [08:42<00:00, 522.19s/it]\n"
     ]
    }
   ],
   "source": [
    "n_executions = 3\n",
    "levels = 3\n",
    "\n",
    "df_humapFLANN_level2 = pd.DataFrame()\n",
    "df_humapFLANN_level0 = pd.DataFrame()\n",
    "\n",
    "df_humapKDTREE_NN_level2 = pd.DataFrame()\n",
    "df_humapKDTREE_NN_level0 = pd.DataFrame()\n",
    "\n",
    "df_hsneCPU_level2 = pd.DataFrame()\n",
    "df_hsneCPU_level0 = pd.DataFrame()\n",
    "\n",
    "df_hsneGPU_level2 = pd.DataFrame()\n",
    "df_hsneGPU_level0 = pd.DataFrame()\n",
    "\n",
    "df_humap_level2 = pd.DataFrame()\n",
    "df_humap_level0 = pd.DataFrame()\n",
    "\n",
    "df_mphate_level2 = pd.DataFrame()\n",
    "df_mphate_level0 = pd.DataFrame()\n",
    "\n",
    "datasets = []\n",
    "\n",
    "# datasets.append({\n",
    "#    'load': load_scRNAseq,\n",
    "#    'name': 'scRNAseq'\n",
    "# })\n",
    "datasets.append({\n",
    "   'load': load_mammals,\n",
    "   'name': 'mammals'\n",
    "})\n",
    "# datasets.append({\n",
    "#     'load': load_fmnist,\n",
    "#     'name': 'fmnist'\n",
    "# })\n",
    "# datasets.append({\n",
    "#     'load': load_mnist,\n",
    "#     'name': 'mnist'\n",
    "# })\n",
    "    \n",
    "    \n",
    "for dataset in datasets:\n",
    "    print(\"Loading %s dataset...\" % (dataset['name']))\n",
    "    X, y = dataset['load']()\n",
    "    print(\"Done.\")\n",
    "    init = 2\n",
    "\n",
    "    print(\"initing with %d\" % (init))\n",
    "#     init = 0\n",
    "\n",
    "#     if not os.path.exists(\"experiments/comparison/\"+dataset['name']):\n",
    "#         os.mkdir(\"experiments/comparison/\"+dataset['name'])\n",
    "        \n",
    "        \n",
    "#     for i in range(n_executions):\n",
    "#         if os.path.exists('experiments/comparison/'+dataset['name']+'/humapKDTREE_NN_it'+str(i)+'_level0.csv'):\n",
    "#             init = i+1\n",
    "          \n",
    "                \n",
    "    print(\"Initing at %d\" % (init))\n",
    "\n",
    "        \n",
    "    level2 = 0\n",
    "    for execution in tqdm(range(init, n_executions)):\n",
    "        \n",
    "        time_file = open(\"experiments/comparison/\"+dataset['name']+'/run-time.csv', 'a')\n",
    "        size_file = open(\"experiments/comparison/\"+dataset['name']+'/mphate_size_level2.csv', 'a')\n",
    "\n",
    "        hsneCPU = nptsne.HSne(True)\n",
    "        tic = time.time()\n",
    "        hsneCPU.create_hsne(X, 3)\n",
    "        execution_hsneCPU_fit = time.time()-tic\n",
    "\n",
    "        hsneGPU = nptsne.HSne(True)\n",
    "        tic = time.time()\n",
    "        hsneGPU.create_hsne(X, 3)\n",
    "        execution_hsneGPU_fit = time.time()-tic\n",
    "\n",
    "        n_level0 = hsneGPU.get_scale(0).num_points\n",
    "        n_level1 = hsneGPU.get_scale(1).num_points\n",
    "        n_level2 = hsneGPU.get_scale(2).num_points\n",
    "\n",
    "        hUmap = h_umap.HUMAP('precomputed', np.array([n_level1/n_level0, n_level2/n_level1]), 100, 0.15, \"NNDescent\", 0.0, True)\n",
    "        hUmap.set_distance_similarity(False)\n",
    "        hUmap.set_path_increment(False)\n",
    "        hUmap.set_influence_neighborhood(0)\n",
    "        \n",
    "        executed_mphate = False\n",
    "        \n",
    "        if dataset['name'] != 'mnist' and dataset['name'] != 'fmnist':\n",
    "            mp_op = mp.Multiscale_PHATE(n_jobs=10)\n",
    "\n",
    "            \"\"\"\n",
    "                Executing Multiscale PHATE\n",
    "            \"\"\"\n",
    "\n",
    "            execution_mphate = -1\n",
    "#             try: \n",
    "            tic = time.time()\n",
    "            levels = mp_op.fit(X)\n",
    "            execution_mphate_fit = time.time()-tic\n",
    "\n",
    "\n",
    "            level2 = 0\n",
    "            dif = np.abs(len(np.unique(mp_op.NxTs[0])) - n_level2)\n",
    "            for level in range(len(levels)):\n",
    "                d = np.abs(len(np.unique(mp_op.NxTs[level])) - n_level2)\n",
    "                if d < dif:\n",
    "                    dif = d\n",
    "                    level2 = level\n",
    "            print(\"level: %d, n: %d, dif: %d\" % (level2, len(np.unique(mp_op.NxTs[level2])), len(np.unique(mp_op.NxTs[level2]))-n_level2))\n",
    "\n",
    "            tic = time.time()\n",
    "            embedding2, _, _ = mp_op.transform(level2, level2)\n",
    "            execution_mphate_level2 = (time.time()-tic)\n",
    "\n",
    "            tic = time.time()\n",
    "            embedding0, _, _ = mp_op.transform(0, 0)\n",
    "            execution_mphate_level0 = (time.time()-tic)\n",
    "\n",
    "            df_mphate_level2['label'+str(execution)] = y[np.unique(mp_op.NxTs[level2])]\n",
    "            df_mphate_level2['x'+str(execution)] = embedding2[:, 0]\n",
    "            df_mphate_level2['y'+str(execution)] = embedding2[:, 1]\n",
    "            df_mphate_level2['inds'+str(execution)] = np.unique(mp_op.NxTs[level2])\n",
    "\n",
    "            df_mphate_level0['label'+str(execution)] = y\n",
    "            df_mphate_level0['x'+str(execution)] = embedding0[:, 0]\n",
    "            df_mphate_level0['y'+str(execution)] = embedding0[:, 1]\n",
    "            df_mphate_level0['inds'+str(execution)] = np.arange(len(y))\n",
    "\n",
    "            df_mphate_level2.to_csv(\"experiments/comparison/\"+dataset['name']+'/mphate_it'+str(execution)+'_level2.csv', index=False)\n",
    "            df_mphate_level0.to_csv(\"experiments/comparison/\"+dataset['name']+'/mphate_it'+str(execution)+'_level0.csv', index=False)\n",
    "            executed_mphate = True\n",
    "#             except:\n",
    "#                 executed_mphate = False\n",
    "#                 print((\"Could not compute Multiscale PHATE embeddings.\"))\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "            Executing HSNE in CPU\n",
    "        \"\"\"\n",
    "        tic = time.time()\n",
    "        container = hsne_analysis.AnalysisModel(hsneCPU, hsne_analysis.EmbedderType.CPU)\n",
    "        analysis_level2 = container.top_analysis\n",
    "        for i in range(500):\n",
    "            analysis_level2.do_iteration()\n",
    "        execution_hsneCPU_level2 = (time.time()-tic)\n",
    "\n",
    "        analysis_level1 = container.add_new_analysis(analysis_level2, np.arange(n_level2))\n",
    "    #     for i in range(500):\n",
    "    #         analysis_level1.do_iteration()\n",
    "        \n",
    "        tic = time.time()\n",
    "        analysis_level0 = container.add_new_analysis(analysis_level1, np.arange(n_level1))\n",
    "        for i in range(500):\n",
    "            analysis_level0.do_iteration()\n",
    "\n",
    "        execution_hsneCPU_level0 = (time.time()-tic)\n",
    "\n",
    "\n",
    "        df_hsneCPU_level2['label'+str(execution)] = y[analysis_level2.landmark_orig_indexes]\n",
    "        df_hsneCPU_level2['x'+str(execution)] = analysis_level2.embedding[:, 0]\n",
    "        df_hsneCPU_level2['y'+str(execution)] = analysis_level2.embedding[:, 1]\n",
    "        df_hsneCPU_level2['inds'+str(execution)] = analysis_level2.landmark_orig_indexes\n",
    "\n",
    "        df_hsneCPU_level0['label'+str(execution)] = y[analysis_level0.landmark_orig_indexes]\n",
    "        df_hsneCPU_level0['x'+str(execution)] = analysis_level0.embedding[:, 0]\n",
    "        df_hsneCPU_level0['y'+str(execution)] = analysis_level0.embedding[:, 1]\n",
    "        df_hsneCPU_level0['inds'+str(execution)] = analysis_level0.landmark_orig_indexes\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "            Executing HSNE in GPU\n",
    "        \"\"\"\n",
    "        tic = time.time()\n",
    "        container = hsne_analysis.AnalysisModel(hsneGPU, hsne_analysis.EmbedderType.GPU)\n",
    "        analysis_level2 = container.top_analysis\n",
    "        for i in range(500):\n",
    "            analysis_level2.do_iteration()\n",
    "        execution_hsneGPU_level2 = (time.time()-tic) \n",
    "\n",
    "        analysis_level1 = container.add_new_analysis(analysis_level2, np.arange(n_level2))\n",
    "    #     for i in range(500):\n",
    "    #         analysis_level1.do_iteration()\n",
    "        \n",
    "        tic = time.time()\n",
    "        analysis_level0 = container.add_new_analysis(analysis_level1, np.arange(n_level1))\n",
    "        for i in range(500):\n",
    "            analysis_level0.do_iteration()\n",
    "\n",
    "        execution_hsneGPU_level0 = (time.time()-tic) \n",
    "\n",
    "\n",
    "        df_hsneGPU_level2['label'+str(execution)] = y[analysis_level2.landmark_orig_indexes]\n",
    "        df_hsneGPU_level2['x'+str(execution)] = analysis_level2.embedding[:, 0]\n",
    "        df_hsneGPU_level2['y'+str(execution)] = analysis_level2.embedding[:, 1]\n",
    "        df_hsneGPU_level2['inds'+str(execution)] = analysis_level2.landmark_orig_indexes\n",
    "\n",
    "        df_hsneGPU_level0['label'+str(execution)] = y[analysis_level0.landmark_orig_indexes]\n",
    "        df_hsneGPU_level0['x'+str(execution)] = analysis_level0.embedding[:, 0]\n",
    "        df_hsneGPU_level0['y'+str(execution)] = analysis_level0.embedding[:, 1]\n",
    "        df_hsneGPU_level0['inds'+str(execution)] = analysis_level0.landmark_orig_indexes\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "            Executing HUMAP\n",
    "        \"\"\"\n",
    "        tic = time.time()\n",
    "        hUmap.fit(X, y)\n",
    "        execution_humap_fit = time.time() - tic\n",
    "        \n",
    "        tic = time.time()\n",
    "        embedding2 = hUmap.transform(2)\n",
    "        execution_humap_level2 = time.time() - tic\n",
    "        \n",
    "        tic = time.time()\n",
    "        embedding0 = hUmap.transform(0)\n",
    "        execution_humap_level0 = time.time() - tic\n",
    "\n",
    "\n",
    "        df_humap_level2['label'+str(execution)] = hUmap.get_labels(2)\n",
    "        df_humap_level2['x'+str(execution)] = embedding2[:, 0]\n",
    "        df_humap_level2['y'+str(execution)] = embedding2[:, 1]\n",
    "        df_humap_level2['inds'+str(execution)] = hUmap.get_original_indices(2)\n",
    "\n",
    "        df_humap_level0['label'+str(execution)] = y\n",
    "        df_humap_level0['x'+str(execution)] = embedding0[:, 0]\n",
    "        df_humap_level0['y'+str(execution)] = embedding0[:, 1]\n",
    "        df_humap_level0['inds'+str(execution)] = np.arange(len(y))\n",
    "        \n",
    "        \n",
    "        hUmapFLANN = h_umap.HUMAP('precomputed', np.array([n_level1/n_level0, n_level2/n_level1]), 100, 0.15, \"FLANN\", 0.0, True)\n",
    "        hUmapFLANN.set_distance_similarity(False)\n",
    "        hUmapFLANN.set_path_increment(False)\n",
    "        hUmapFLANN.set_influence_neighborhood(0)\n",
    "        \n",
    "\n",
    "        tic = time.time()\n",
    "        hUmapFLANN.fit(X, y)\n",
    "        execution_humapFLANN_fit = time.time() - tic\n",
    "        \n",
    "        tic = time.time()\n",
    "        embedding2 = hUmapFLANN.transform(2)\n",
    "        execution_humapFLANN_level2 = time.time() - tic\n",
    "            \n",
    "        tic = time.time()\n",
    "        embedding0 = hUmapFLANN.transform(0)\n",
    "        execution_humapFLANN_level0 = time.time() - tic\n",
    "\n",
    "        df_humapFLANN_level2['label'+str(execution)] = hUmapFLANN.get_labels(2)\n",
    "        df_humapFLANN_level2['x'+str(execution)] = embedding2[:, 0]\n",
    "        df_humapFLANN_level2['y'+str(execution)] = embedding2[:, 1]\n",
    "        df_humapFLANN_level2['inds'+str(execution)] = hUmapFLANN.get_original_indices(2)\n",
    "\n",
    "        df_humapFLANN_level0['label'+str(execution)] = y\n",
    "        df_humapFLANN_level0['x'+str(execution)] = embedding0[:, 0]\n",
    "        df_humapFLANN_level0['y'+str(execution)] = embedding0[:, 1]\n",
    "        df_humapFLANN_level0['inds'+str(execution)] = np.arange(len(y))\n",
    "        \n",
    "        \n",
    "        hUmapKDTREE_NN = h_umap.HUMAP('precomputed', np.array([n_level1/n_level0, n_level2/n_level1]), 100, 0.15, \"KDTree_NNDescent\", 0.0, True)\n",
    "        hUmapKDTREE_NN.set_distance_similarity(False)\n",
    "        hUmapKDTREE_NN.set_path_increment(False)\n",
    "        hUmapKDTREE_NN.set_influence_neighborhood(0)\n",
    "        \n",
    "\n",
    "        tic = time.time()\n",
    "\n",
    "        hUmapKDTREE_NN.fit(X, y)\n",
    "        execution_humapKDTREE_NN_fit = time.time() - tic\n",
    "        \n",
    "        tic = time.time()\n",
    "        embedding2 = hUmapKDTREE_NN.transform(2)\n",
    "        execution_humapKDTREE_NN_level2 = time.time() - tic\n",
    "        \n",
    "        embedding0 = hUmapKDTREE_NN.transform(0)\n",
    "        execution_humapKDTREE_NN_level0 = time.time() - tic\n",
    "\n",
    "        df_humapKDTREE_NN_level2['label'+str(execution)] = hUmapKDTREE_NN.get_labels(2)\n",
    "        df_humapKDTREE_NN_level2['x'+str(execution)] = embedding2[:, 0]\n",
    "        df_humapKDTREE_NN_level2['y'+str(execution)] = embedding2[:, 1]\n",
    "        df_humapKDTREE_NN_level2['inds'+str(execution)] = hUmapKDTREE_NN.get_original_indices(2)\n",
    "\n",
    "        df_humapKDTREE_NN_level0['label'+str(execution)] = y\n",
    "        df_humapKDTREE_NN_level0['x'+str(execution)] = embedding0[:, 0]\n",
    "        df_humapKDTREE_NN_level0['y'+str(execution)] = embedding0[:, 1]\n",
    "        df_humapKDTREE_NN_level0['inds'+str(execution)] = np.arange(len(y))\n",
    "        \n",
    "\n",
    "        df_hsneCPU_level2.to_csv(\"experiments/comparison/\"+dataset['name']+'/hsneCPU_it'+str(execution)+'_level2.csv', index=False)\n",
    "        df_hsneCPU_level0.to_csv(\"experiments/comparison/\"+dataset['name']+'/hsneCPU_it'+str(execution)+'_level0.csv', index=False)\n",
    "        df_hsneGPU_level2.to_csv(\"experiments/comparison/\"+dataset['name']+'/hsneGPU_it'+str(execution)+'_level2.csv', index=False)\n",
    "        df_hsneGPU_level0.to_csv(\"experiments/comparison/\"+dataset['name']+'/hsneGPU_it'+str(execution)+'_level0.csv', index=False)\n",
    "        df_humap_level2.to_csv(\"experiments/comparison/\"+dataset['name']+'/humap_it'+str(execution)+'_level2.csv', index=False)\n",
    "        df_humap_level0.to_csv(\"experiments/comparison/\"+dataset['name']+'/humap_it'+str(execution)+'_level0.csv', index=False)\n",
    "        df_humapFLANN_level2.to_csv(\"experiments/comparison/\"+dataset['name']+'/humapFLANN_it'+str(execution)+'_level2.csv', index=False)\n",
    "        df_humapFLANN_level0.to_csv(\"experiments/comparison/\"+dataset['name']+'/humapFLANN_it'+str(execution)+'_level0.csv', index=False)\n",
    "        df_humapKDTREE_NN_level2.to_csv(\"experiments/comparison/\"+dataset['name']+'/humapKDTREE_NN_it'+str(execution)+'_level2.csv', index=False)\n",
    "        df_humapKDTREE_NN_level0.to_csv(\"experiments/comparison/\"+dataset['name']+'/humapKDTREE_NN_it'+str(execution)+'_level0.csv', index=False)        \n",
    "        \n",
    "        df_hsneCPU_level2 = pd.DataFrame()\n",
    "        df_hsneCPU_level0 = pd.DataFrame()\n",
    "\n",
    "        df_hsneGPU_level2 = pd.DataFrame()\n",
    "        df_hsneGPU_level0 = pd.DataFrame()\n",
    "\n",
    "        df_humap_level2 = pd.DataFrame()\n",
    "        df_humap_level0 = pd.DataFrame()\n",
    "\n",
    "        df_mphate_level2 = pd.DataFrame()\n",
    "        df_mphate_level0 = pd.DataFrame()\n",
    "        \n",
    "        df_humapFLANN_level2 = pd.DataFrame()\n",
    "        df_humapFLANN_level0 = pd.DataFrame()\n",
    "        \n",
    "        df_humapKDTREE_NN_level2 = pd.DataFrame()\n",
    "        df_humapKDTREE_NN_level0 = pd.DataFrame()\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        time_file.write('HSNE CPU,Fit,'+str(execution_hsneCPU_fit)+'\\n')\n",
    "        time_file.write('HSNE GPU,Fit,'+str(execution_hsneGPU_fit)+'\\n')\n",
    "        time_file.write('HUMAP,Fit,'+str(execution_humap_fit)+'\\n')        \n",
    "        time_file.write('HUMAP FLANN,Fit,'+str(execution_humapFLANN_fit)+'\\n')\n",
    "        time_file.write('HUMAP KDTree + NNDescent,Fit,'+str(execution_humapKDTREE_NN_fit)+'\\n')\n",
    "        \n",
    "        time_file.write('HSNE CPU,Level 2,'+str(execution_hsneCPU_level2)+'\\n')\n",
    "        time_file.write('HSNE GPU,Level 2,'+str(execution_hsneGPU_level2)+'\\n')\n",
    "        time_file.write('HUMAP,Level 2,'+str(execution_humap_level2)+'\\n')\n",
    "        time_file.write('HUMAP FLANN,Level 2,'+str(execution_humapFLANN_level2)+'\\n')\n",
    "        time_file.write('HUMAP KDTree + NNDescent,Level 2,'+str(execution_humapKDTREE_NN_level2)+'\\n')\n",
    "        \n",
    "        time_file.write('HSNE CPU,Level 0,'+str(execution_hsneCPU_level2)+'\\n')\n",
    "        time_file.write('HSNE GPU,Level 0,'+str(execution_hsneGPU_level2)+'\\n')\n",
    "        time_file.write('HUMAP,Level 0,'+str(execution_humap_level2)+'\\n')\n",
    "        time_file.write('HUMAP FLANN,Level 0,'+str(execution_humapFLANN_level0)+'\\n')\n",
    "        time_file.write('HUMAP KDTree + NNDescent,Level 0,'+str(execution_humapKDTREE_NN_level0)+'\\n')\n",
    "        \n",
    "        \n",
    "        if executed_mphate:\n",
    "            time_file.write('Multiscale PHATE,Fit,'+str(execution_mphate_level2)+'\\n')\n",
    "            time_file.write('Multiscale PHATE,Level 2,'+str(execution_mphate_level2)+'\\n')\n",
    "            time_file.write('Multiscale PHATE,Level 0,'+str(execution_mphate_level0)+'\\n')\n",
    "            size_file.write(str(level2)+','+str(len(np.unique(mp_op.NxTs[level2])))+'\\n')\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        size_file.close()\n",
    "        time_file.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "#     df_times = pd.DataFrame({\n",
    "#         'HSNE CPU': hsneCPU_time,\n",
    "#         'HSNE GPU': hsneGPU_time,\n",
    "#         'HUMAP': humap_time,\n",
    "#         'Multiscale PHATE': mphate_time\n",
    "#     })\n",
    "    \n",
    "#     df_sizes = pd.DataFrame({\n",
    "#         'Size': mphate_sizes\n",
    "#     })\n",
    "    \n",
    "    \n",
    "#     df_times.to_csv(\"experiments/comparison/\"+dataset['name']+'/time_execution.csv', index=False)\n",
    "#     df_sizes.to_csv(\"experiments/comparison/\"+dataset['name']+'/mphate_size_level2.csv', index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testar HUMAP com FAISS, KDtree e FLANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_hsneCPU_level0['x0'].values, df_hsneCPU_level0['y0'].values, c=df_hsneCPU_level0['label0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_hsneGPU_level2['x0'].values, df_hsneGPU_level2['y0'].values, c=df_hsneGPU_level2['label0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_hsneGPU_level0['x0'].values, df_hsneGPU_level0['y0'].values, c=df_hsneGPU_level0['label0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_humap_level2['x0'].values, df_humap_level2['y0'].values, c=df_humap_level2['label0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_humap_level0['x0'].values, df_humap_level0['y0'].values, c=df_humap_level0['label0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_mphate_level2['x0'].values, df_mphate_level2['y0'].values, c=df_mphate_level2['label0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_mphate_level0['x0'].values, df_mphate_level0['y0'].values, c=df_mphate_level0['label0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
